{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 - Vision Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms.v2 import (\n",
    "    Compose,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    ToImage,\n",
    "    ToDtype,\n",
    "    Normalize,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        RandomHorizontalFlip(p=0.5),\n",
    "        RandomResizedCrop(size=(32, 32), scale=(0.8, 1), ratio=(0.9, 1.1)),\n",
    "        ToImage(),\n",
    "        ToDtype(torch.float32, scale=True),\n",
    "        Normalize(mean=[0, 0, 0], std=[1, 1, 1]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_cifar = torchvision.datasets.CIFAR10(\n",
    "    \"./lab6_data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "test_transform =Compose(\n",
    "    [\n",
    "        ToImage(),\n",
    "        ToDtype(torch.float32, scale=True),\n",
    "        Normalize(mean=[0, 0, 0], std=[1, 1, 1]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "test_cifar = torchvision.datasets.CIFAR10(\"./lab6_data\", train=False, download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cifar[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need for seperate function for cutting image into patches, we can do it with single `view()` call inside `forward()` method of our model.\n",
    "On another note, I'm not using special CLS token for classification but rather calculate average of all other tokens.\n",
    "I've also made a head a little bit bigger (added one extra Linear Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class ClassifierCIFAR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassifierCIFAR, self).__init__()\n",
    "\n",
    "        self.no_of_classes = 10\n",
    "        self.input_size = 32\n",
    "        self.patch_size = 4\n",
    "        self.sentence_length = (self.input_size // self.patch_size) ** 2\n",
    "        self.embedding_size = 256\n",
    "        self.no_of_transformers = 6\n",
    "        self.dropout = 0.2\n",
    "\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            3,\n",
    "            self.embedding_size,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            padding=0,\n",
    "        )\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(self.embedding_size, self.dropout)\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=self.embedding_size,\n",
    "                nhead=8,\n",
    "                dim_feedforward=512,\n",
    "                dropout=self.dropout,\n",
    "                activation=\"gelu\",\n",
    "                norm_first=True,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            self.no_of_transformers,\n",
    "        )\n",
    "\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.LayerNorm(self.embedding_size),\n",
    "            nn.Linear(self.embedding_size, self.embedding_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.embedding_size, self.no_of_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        # embedding patches using convolution, flattening them and putting dimensions in right order\n",
    "        tokens = self.patch_embedding(\n",
    "            x\n",
    "        ).view(b, self.embedding_size, self.sentence_length).permute(0, 2, 1) \n",
    "\n",
    "        # adding positional encoding\n",
    "        tokens = tokens + self.positional_encoding(tokens)\n",
    "\n",
    "        features = self.transformer(tokens)\n",
    "\n",
    "        # extracting CLS tokens and passing to feedforward\n",
    "        out = self.feedforward(features.mean(dim=1))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 786.0145736694336, time: 3.23min\n",
      "Epoch: 7, Loss: 789.532921218872, time: 7.48min\n",
      "Epoch: 8, Loss: 781.4656314849854, time: 11.68min\n",
      "Epoch: 9, Loss: 772.4646377563477, time: 15.90min\n",
      "Epoch: 10, Loss: 769.9245090484619, time: 20.09min\n",
      "Epoch: 11, Loss: 764.606177520752, time: 24.23min\n",
      "Epoch: 12, Loss: 762.1738498687744, time: 28.20min\n",
      "Epoch: 13, Loss: 754.0347030639648, time: 32.24min\n",
      "Epoch: 14, Loss: 749.935619354248, time: 35.89min\n",
      "Epoch: 15, Loss: 750.7913146972656, time: 38.96min\n",
      "Epoch: 16, Loss: 747.3842060089112, time: 41.60min\n",
      "Epoch: 17, Loss: 752.7074794769287, time: 44.26min\n",
      "Epoch: 18, Loss: 744.342322921753, time: 46.96min\n",
      "Epoch: 19, Loss: 762.3053482055664, time: 49.80min\n",
      "Epoch: 20, Loss: 789.1593585968018, time: 52.62min\n",
      "Epoch: 21, Loss: 757.5073719024658, time: 55.38min\n",
      "Epoch: 22, Loss: 757.5611419677734, time: 58.16min\n",
      "Epoch: 23, Loss: 748.5347995758057, time: 61.00min\n",
      "Epoch: 24, Loss: 744.0100494384766, time: 63.75min\n",
      "Epoch: 25, Loss: 764.5131568908691, time: 66.50min\n",
      "Epoch: 26, Loss: 781.5429649353027, time: 69.29min\n",
      "Epoch: 27, Loss: 776.7279457092285, time: 72.07min\n",
      "Epoch: 28, Loss: 761.74408493042, time: 74.80min\n",
      "Epoch: 29, Loss: 769.5985614776612, time: 77.58min\n",
      "Epoch: 30, Loss: 770.3300025939941, time: 80.40min\n",
      "Epoch: 31, Loss: 797.3756256103516, time: 83.26min\n",
      "Epoch: 32, Loss: 775.7487945556641, time: 85.99min\n",
      "Epoch: 33, Loss: 764.790156173706, time: 88.78min\n",
      "Epoch: 34, Loss: 757.5734966278076, time: 91.66min\n",
      "Epoch: 35, Loss: 752.7918800354004, time: 94.46min\n",
      "Epoch: 36, Loss: 754.3191131591797, time: 97.18min\n",
      "Epoch: 37, Loss: 757.6325218200684, time: 99.93min\n",
      "Epoch: 38, Loss: 760.4335758209229, time: 102.69min\n",
      "Epoch: 39, Loss: 750.9249172210693, time: 105.43min\n",
      "Epoch: 40, Loss: 744.6008068084717, time: 108.11min\n",
      "Epoch: 41, Loss: 741.5508289337158, time: 110.81min\n",
      "Epoch: 42, Loss: 747.9461833953858, time: 113.51min\n",
      "Epoch: 43, Loss: 739.6688186645508, time: 116.28min\n",
      "Epoch: 44, Loss: 737.5653987884522, time: 119.01min\n",
      "Epoch: 45, Loss: 738.7462959289551, time: 121.70min\n",
      "Epoch: 46, Loss: 737.36237449646, time: 124.39min\n",
      "Epoch: 47, Loss: 730.4797775268555, time: 127.10min\n",
      "Epoch: 48, Loss: 725.0452018737793, time: 129.85min\n",
      "Epoch: 49, Loss: 732.0208744049072, time: 132.58min\n",
      "Epoch: 50, Loss: 730.4215656280518, time: 135.90min\n",
      "Epoch: 51, Loss: 727.2295143127442, time: 138.85min\n",
      "Epoch: 52, Loss: 724.5974086761474, time: 141.59min\n",
      "Epoch: 53, Loss: 726.4299320220947, time: 144.38min\n",
      "Epoch: 54, Loss: 722.0266845703125, time: 147.10min\n",
      "Epoch: 55, Loss: 723.3304122924804, time: 149.89min\n",
      "Epoch: 56, Loss: 721.303426361084, time: 152.62min\n",
      "Epoch: 57, Loss: 713.996505355835, time: 155.32min\n",
      "Epoch: 58, Loss: 710.050517654419, time: 158.03min\n",
      "Epoch: 59, Loss: 703.5367031097412, time: 160.75min\n",
      "Epoch: 60, Loss: 699.4884952545166, time: 163.46min\n",
      "Epoch: 61, Loss: 705.9451885223389, time: 166.13min\n",
      "Epoch: 62, Loss: 708.298677444458, time: 168.82min\n",
      "Epoch: 63, Loss: 705.2856605529785, time: 171.59min\n",
      "Epoch: 64, Loss: 702.9468139648437, time: 174.31min\n",
      "Epoch: 65, Loss: 699.9676612854004, time: 176.97min\n",
      "Epoch: 66, Loss: 718.5705905914307, time: 179.67min\n",
      "Epoch: 67, Loss: 729.1677585601807, time: 182.39min\n",
      "Epoch: 68, Loss: 720.7673030853272, time: 185.09min\n",
      "Epoch: 69, Loss: 721.919412612915, time: 187.77min\n",
      "Epoch: 70, Loss: 723.3743503570556, time: 190.53min\n",
      "Epoch: 71, Loss: 716.4752479553223, time: 193.32min\n",
      "Epoch: 72, Loss: 735.5040157318115, time: 196.04min\n",
      "Epoch: 73, Loss: 734.2606708526612, time: 198.73min\n",
      "Epoch: 74, Loss: 718.012020111084, time: 201.41min\n",
      "Epoch: 75, Loss: 712.124499130249, time: 204.08min\n",
      "Epoch: 76, Loss: 711.0911430358886, time: 206.79min\n",
      "Epoch: 77, Loss: 709.6910816192627, time: 209.41min\n",
      "Epoch: 78, Loss: 706.9590587615967, time: 212.07min\n",
      "Epoch: 79, Loss: 706.5457942962646, time: 214.70min\n",
      "Epoch: 80, Loss: 704.2157642364502, time: 217.40min\n",
      "Epoch: 81, Loss: 701.9255195617676, time: 220.06min\n",
      "Epoch: 82, Loss: 702.3345726013183, time: 222.74min\n",
      "Epoch: 83, Loss: 700.4668449401855, time: 225.36min\n",
      "Epoch: 84, Loss: 699.0740051269531, time: 228.05min\n",
      "Epoch: 85, Loss: 698.6576091766358, time: 230.71min\n",
      "Epoch: 86, Loss: 698.5734935760498, time: 233.36min\n",
      "Epoch: 87, Loss: 694.9069709777832, time: 236.01min\n",
      "Epoch: 88, Loss: 695.044584274292, time: 238.69min\n",
      "Epoch: 89, Loss: 695.878020477295, time: 241.31min\n",
      "Epoch: 90, Loss: 694.4093154907226, time: 243.97min\n",
      "Epoch: 91, Loss: 693.4436019897461, time: 246.65min\n",
      "Epoch: 92, Loss: 692.571312713623, time: 249.33min\n",
      "Epoch: 93, Loss: 692.0822814941406, time: 251.97min\n",
      "Epoch: 94, Loss: 691.0685131072999, time: 254.68min\n",
      "Epoch: 95, Loss: 689.410326385498, time: 257.33min\n",
      "Epoch: 96, Loss: 689.0114650726318, time: 259.99min\n",
      "Epoch: 97, Loss: 690.858959197998, time: 262.67min\n",
      "Epoch: 98, Loss: 688.4844326019287, time: 265.34min\n",
      "Epoch: 99, Loss: 684.6994846343994, time: 268.00min\n",
      "Epoch: 100, Loss: 684.6044921875, time: 270.65min\n",
      "Epoch: 101, Loss: 683.5379081726074, time: 273.31min\n",
      "Epoch: 102, Loss: 683.1709552764893, time: 276.09min\n",
      "Epoch: 103, Loss: 681.3631038665771, time: 278.72min\n",
      "Epoch: 104, Loss: 682.3493705749512, time: 281.36min\n",
      "Epoch: 105, Loss: 679.2716899871826, time: 284.06min\n",
      "Epoch: 106, Loss: 679.7635292053222, time: 286.77min\n",
      "Epoch: 107, Loss: 680.7940410614013, time: 289.50min\n",
      "Epoch: 108, Loss: 680.4070358276367, time: 292.20min\n",
      "Epoch: 109, Loss: 676.4211757659912, time: 294.87min\n",
      "Epoch: 110, Loss: 677.2108646392822, time: 297.58min\n",
      "Epoch: 111, Loss: 677.0067596435547, time: 300.24min\n",
      "Epoch: 112, Loss: 674.6626399993896, time: 303.03min\n",
      "Epoch: 113, Loss: 687.2022651672363, time: 305.70min\n",
      "Epoch: 114, Loss: 686.3393882751465, time: 308.39min\n",
      "Epoch: 115, Loss: 683.7424335479736, time: 311.11min\n",
      "Epoch: 116, Loss: 678.8040935516358, time: 313.82min\n",
      "Epoch: 117, Loss: 678.3299701690673, time: 316.55min\n",
      "Epoch: 118, Loss: 675.7966201782226, time: 319.27min\n",
      "Epoch: 119, Loss: 676.7919612884522, time: 322.01min\n",
      "Epoch: 120, Loss: 676.0855018615723, time: 324.64min\n",
      "Epoch: 121, Loss: 670.6332321166992, time: 327.30min\n",
      "Epoch: 122, Loss: 671.6645706176757, time: 329.95min\n",
      "Epoch: 123, Loss: 669.160266494751, time: 332.63min\n",
      "Epoch: 124, Loss: 668.3861679077148, time: 335.30min\n",
      "Epoch: 125, Loss: 667.9683120727539, time: 338.04min\n",
      "Epoch: 126, Loss: 666.1218997955323, time: 340.73min\n",
      "Epoch: 127, Loss: 663.4227836608886, time: 343.42min\n",
      "Epoch: 128, Loss: 663.0245651245117, time: 346.13min\n",
      "Epoch: 129, Loss: 663.7704208374023, time: 348.83min\n",
      "Epoch: 130, Loss: 662.8820331573486, time: 351.51min\n",
      "Epoch: 131, Loss: 660.2649688720703, time: 354.20min\n",
      "Epoch: 132, Loss: 660.2059505462646, time: 356.90min\n",
      "Epoch: 133, Loss: 660.7798587799073, time: 359.58min\n",
      "Epoch: 134, Loss: 661.3352188110351, time: 362.34min\n",
      "Epoch: 135, Loss: 661.4801914215088, time: 365.05min\n",
      "Epoch: 136, Loss: 660.4629348754883, time: 367.75min\n",
      "Epoch: 137, Loss: 660.1807853698731, time: 370.52min\n",
      "Epoch: 138, Loss: 663.2795597076416, time: 373.23min\n",
      "Epoch: 139, Loss: 659.6257266998291, time: 375.92min\n",
      "Epoch: 140, Loss: 662.2467178344726, time: 378.68min\n",
      "Epoch: 141, Loss: 662.0222774505615, time: 381.41min\n",
      "Epoch: 142, Loss: 663.2043518066406, time: 384.09min\n",
      "Epoch: 143, Loss: 662.363865661621, time: 386.87min\n",
      "Epoch: 144, Loss: 663.7972995758057, time: 389.56min\n",
      "Epoch: 145, Loss: 662.7069751739502, time: 392.28min\n",
      "Epoch: 146, Loss: 662.2698970794678, time: 395.03min\n",
      "Epoch: 147, Loss: 661.7352172851563, time: 397.75min\n",
      "Epoch: 148, Loss: 660.619873046875, time: 400.51min\n",
      "Epoch: 149, Loss: 656.8826099395752, time: 403.23min\n",
      "Epoch: 150, Loss: 656.129920578003, time: 405.99min\n",
      "Epoch: 151, Loss: 656.2923236846924, time: 408.74min\n",
      "Epoch: 152, Loss: 654.1927619934082, time: 411.48min\n",
      "Epoch: 153, Loss: 656.3238353729248, time: 414.24min\n",
      "Epoch: 154, Loss: 656.3962753295898, time: 417.00min\n",
      "Epoch: 155, Loss: 653.8643058776855, time: 419.68min\n",
      "Epoch: 156, Loss: 655.4656158447266, time: 422.40min\n",
      "Epoch: 157, Loss: 655.6257610321045, time: 425.11min\n",
      "Epoch: 158, Loss: 655.0722671508789, time: 427.86min\n",
      "Epoch: 159, Loss: 653.3594585418701, time: 430.67min\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 400 # largest I could fit into my GPU\n",
    "EPOCHS = 160\n",
    "START_EPOCH = 6\n",
    "model = ClassifierCIFAR().to(\"cuda\")\n",
    "model.train()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=75, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# loading from previous runs\n",
    "data = torch.load('lab6_data/model_7')\n",
    "model.load_state_dict(data)\n",
    "data = torch.load('lab6_data/optimizer_7')\n",
    "scheduler.load_state_dict(data['scheduler'])\n",
    "optimizer.load_state_dict(data['optimizer'])\n",
    "\n",
    "train_dataloader = DataLoader(train_cifar, batch_size=BATCH_SIZE, shuffle=True)\n",
    "start_time = time.time()\n",
    "for epoch in  range(START_EPOCH, EPOCHS):\n",
    "    cum_loss = 0\n",
    "    for data, label in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        data, label = data.to(\"cuda\"), label.to(\"cuda\")\n",
    "\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cum_loss += loss.detach().cpu().item() * data.shape[0]\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f'Epoch: {epoch}, Loss: {cum_loss / len(train_dataloader)}, time: {(time.time() - start_time) / 60:.2f}min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"lab6_data/model_{epoch}\")\n",
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": epoch,\n",
    "        \"loss\": loss,\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict(),\n",
    "    },\n",
    "    f\"lab6_data/optimizer_{epoch}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 9, 8, 2, 7, 7, 9, 8, 9, 6, 0, 2, 0, 8, 1, 1, 1, 6, 7, 6, 1, 6, 5,\n",
      "        4, 0, 1, 7, 0, 5, 6, 6, 8, 7, 5, 2, 9, 7, 4, 5, 2, 7, 5, 2, 3, 9, 1, 6,\n",
      "        9, 5, 7, 8, 0, 6, 1, 0, 9, 8, 8, 9, 9, 5, 5, 7, 4, 1, 6, 5, 1, 9, 7, 5,\n",
      "        1, 0, 8, 1, 0, 8, 5, 6, 6, 6, 1, 3, 7, 8, 9, 0, 6, 9, 3, 5, 9, 9, 0, 0,\n",
      "        6, 7, 9, 3, 9, 7, 7, 6, 8, 4, 5, 1, 9, 7, 6, 6, 6, 4, 1, 9, 1, 0, 2, 9,\n",
      "        5, 8, 3, 5, 4, 1, 7, 9, 1, 3, 1, 0, 7, 7, 8, 4, 6, 9, 6, 9, 0, 5, 4, 0,\n",
      "        5, 5, 9, 9, 0, 8, 4, 0, 0, 7, 0, 4, 4, 1, 4, 2, 0, 4, 7, 7, 0, 8, 9, 7,\n",
      "        5, 0, 3, 7, 0, 6, 8, 1, 6, 8, 7, 8, 9, 5, 6, 7, 0, 1, 4, 1, 6, 5, 8, 1,\n",
      "        1, 3, 5, 9, 6, 9, 6, 0, 6, 9, 7, 0, 3, 2, 8, 9, 6, 6, 0, 4, 4, 5, 8, 9,\n",
      "        6, 6, 4, 7, 7, 5, 6, 4, 1, 1, 9, 9, 0, 7, 9, 7, 0, 9, 0, 4, 6, 3, 9, 1,\n",
      "        9, 6, 2, 2, 2, 8, 3, 3, 6, 3, 1, 7, 2, 2, 4, 2, 5, 1, 6, 0, 6, 4, 1, 6,\n",
      "        2, 6, 6, 5, 3, 9, 7, 4, 6, 1, 0, 6, 0, 4, 7, 6, 4, 5, 0, 9, 1, 9, 9, 1,\n",
      "        9, 7, 6, 7, 2, 4, 7, 9, 7, 0, 1, 7, 5, 1, 0, 5, 2, 6, 6, 5, 5, 2, 4, 6,\n",
      "        2, 2, 9, 4, 5, 6, 3, 3, 0, 5, 1, 8, 0, 9, 0, 6, 5, 3, 3, 9, 6, 8, 5, 0,\n",
      "        6, 0, 8, 6, 7, 3, 1, 1, 0, 6, 1, 5, 0, 6, 6, 4, 1, 2, 0, 6, 7, 0, 4, 9,\n",
      "        6, 9, 6, 8, 8, 8, 0, 5, 9, 3, 2, 5, 6, 5, 9, 0, 4, 4, 8, 1, 0, 6, 2, 8,\n",
      "        0, 1, 9, 6, 7, 7, 7, 2, 8, 8, 2, 0, 7, 4, 7, 6], device='cuda:0')\n",
      "tensor([0, 4, 9, 2, 3, 7, 7, 9, 2, 5, 7, 2, 2, 2, 8, 4, 1, 1, 6, 7, 4, 0, 4, 7,\n",
      "        0, 1, 1, 2, 7, 5, 2, 2, 0, 2, 5, 7, 9, 4, 1, 2, 4, 8, 3, 2, 1, 8, 1, 6,\n",
      "        8, 5, 5, 4, 2, 6, 1, 0, 2, 1, 1, 9, 0, 3, 4, 7, 8, 1, 6, 2, 1, 7, 6, 7,\n",
      "        1, 0, 8, 3, 0, 8, 3, 2, 5, 3, 9, 3, 4, 6, 9, 0, 6, 1, 3, 4, 8, 9, 1, 4,\n",
      "        5, 1, 9, 3, 7, 7, 5, 2, 8, 9, 3, 5, 9, 1, 2, 4, 9, 4, 1, 3, 1, 0, 5, 8,\n",
      "        9, 0, 5, 6, 6, 7, 5, 9, 0, 9, 1, 2, 5, 7, 8, 6, 2, 2, 4, 8, 0, 5, 6, 9,\n",
      "        1, 9, 2, 9, 9, 0, 6, 8, 1, 2, 3, 4, 4, 1, 4, 2, 1, 4, 7, 3, 4, 9, 1, 7,\n",
      "        5, 0, 8, 7, 9, 4, 8, 9, 6, 0, 4, 8, 1, 6, 2, 7, 0, 1, 6, 2, 1, 2, 0, 1,\n",
      "        9, 3, 3, 8, 5, 1, 3, 0, 6, 0, 2, 2, 3, 0, 8, 8, 9, 6, 1, 3, 7, 5, 8, 9,\n",
      "        6, 4, 2, 7, 3, 5, 4, 6, 8, 1, 9, 1, 0, 7, 9, 7, 0, 9, 0, 3, 4, 3, 8, 5,\n",
      "        0, 6, 5, 3, 6, 2, 5, 7, 6, 6, 1, 7, 2, 4, 4, 4, 2, 9, 7, 2, 3, 3, 9, 6,\n",
      "        8, 4, 6, 4, 7, 0, 3, 4, 6, 1, 0, 6, 0, 4, 1, 5, 2, 5, 4, 8, 9, 9, 8, 1,\n",
      "        1, 6, 6, 4, 2, 4, 4, 9, 5, 8, 4, 8, 2, 1, 6, 3, 6, 4, 7, 0, 2, 2, 4, 6,\n",
      "        9, 2, 1, 4, 5, 5, 3, 3, 1, 5, 9, 8, 2, 9, 8, 7, 5, 3, 3, 1, 7, 9, 5, 0,\n",
      "        6, 9, 7, 6, 6, 7, 3, 0, 1, 3, 4, 5, 0, 6, 5, 6, 9, 2, 0, 5, 6, 2, 5, 0,\n",
      "        6, 9, 5, 2, 8, 0, 3, 2, 9, 7, 6, 9, 8, 3, 7, 0, 3, 4, 8, 8, 0, 2, 8, 8,\n",
      "        8, 1, 9, 4, 7, 7, 7, 7, 0, 8, 2, 4, 7, 4, 7, 2])\n",
      "Counter({6: 62, 0: 51, 9: 51, 7: 45, 1: 42, 5: 39, 4: 33, 8: 32, 2: 25, 3: 20})\n",
      "Model prediction: airplane\n",
      "Ground truth: airplane\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_cifar, batch_size=BATCH_SIZE, shuffle=True)\n",
    "for img, label in test_dataloader:\n",
    "    plt.imshow(img[0].permute(1,2,0).cpu().numpy())\n",
    "    with torch.inference_mode():\n",
    "        print(torch.argmax(model(img.to(\"cuda\")), 1))\n",
    "        print(label)\n",
    "        print(Counter(list(torch.argmax(model(img.to(\"cuda\")), 1).detach().cpu().numpy())))\n",
    "        print(f'Model prediction: {cifar10_classes[torch.argmax(model(img.to(\"cuda\")), 1)[0]]}')\n",
    "        print(f'Ground truth: {cifar10_classes[label[0]]}') \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.374\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_cifar, batch_size=400, shuffle=True)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    TP = 0\n",
    "\n",
    "    for data, labels in test_dataloader:\n",
    "        data, labels = data.to(\"cuda\"), labels.to(\"cuda\")\n",
    "        output = model(data)\n",
    "        for o, l in zip(torch.argmax(output, 1), labels):\n",
    "            if o == l:\n",
    "                TP += 1\n",
    "\n",
    "    print(f\"Accuracy: {TP / len(test_dataloader) / BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 10 classes, so we can see the classifier was trained. Investigating loss during training we can see that model was still capable of learning more, because the loss was decreasing. Maybe we should use more epochs, or choose better learning rate schedule to train the model faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
